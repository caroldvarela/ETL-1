{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import Notebook - Proyect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this first notebook. In this notebook, we will focus on connecting to the database and creating the necessary tables. Specifically, we will connect locally to a PostgreSQL instance. Afterward, we will perform some brief transformations on our dataset, which is initially in a CSV format, to prepare it for insertion into the tables we created earlier.\n",
    "\n",
    "Before proceeding, ensure that you have already installed the necessary dependencies listed in the requirements.txt file. You can do this by running the following command:\n",
    "\n",
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Workdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that you already have your own .env file containing your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "''' Esta es la config para cuando no estas usando docker \n",
    "work_dir = os.getenv('WORK_DIR')\n",
    "sys.path.append(work_dir)\n",
    "'''\n",
    "# Cambia temporalmente WORK_DIR dentro del notebook\n",
    "os.environ['WORK_DIR'] = '/home/jovyan/work'\n",
    "sys.path.append(os.getenv('WORK_DIR'))\n",
    "sys.path.append(f\"{os.getenv('WORK_DIR')}/src\")\n",
    "sys.path.append(f\"{os.getenv('WORK_DIR')}/transform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.models import CardioTrain\n",
    "from src.database.dbconnection import getconnection\n",
    "from sqlalchemy import inspect\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from transform.TransformData import DataTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SQLAlchemy library, connect to the database. If you encounter any issues, check that your .env file contains the correct environment variables and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conected successfully to database airflow!\n"
     ]
    }
   ],
   "source": [
    "engine = getconnection()\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, this check if the candidates table exists. If a CardioTrain table is already present, it will be dropped. Therefore, be careful not to execute this code in a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if inspect(engine).has_table('CardioTrain'):\n",
    "        CardioTrain.__table__.drop(engine)\n",
    "    CardioTrain.__table__.create(engine)\n",
    "    print(\"Table created successfully.\")\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "finally:\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,  the original dataset is already normalize so we dont need to add things like id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    file = DataTransform('../../data/cardio_train.csv')\n",
    "    file.df.to_sql('CardioTrain', con=engine, if_exists='append', index=False)\n",
    "    print(\"Data uploaded\")\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Database error: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    if hasattr(engine, 'dispose'):\n",
    "        engine.dispose()\n",
    "\n",
    "    if 'session' in locals():\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything was successful, a message should appear confirming that the data was successfully uploaded to our database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data uploaded successfully! \n",
    "\n",
    "Now you can proceed with the next notebook: [002_EDA_dataset.ipynb](../EDA/002_EDA_dataset.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
